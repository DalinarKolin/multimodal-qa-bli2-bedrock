{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98758a80-18f4-4fee-807d-1bb169d54cff",
   "metadata": {},
   "source": [
    "# Multimodal QA with BLIP-2 and Bedrock\n",
    "\n",
    "> *This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08442988-0899-4bbb-8492-99f78e98fd50",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Context\n",
    "Multimodal question-answering (QA) systems are gaining prominence as they can comprehend both text and visual information. Multimodal content is prevalent in today's digital landscape, often found in documents such as PDFs that combine text and images. Traditional OCR models cannot generate descriptions or context for images. While OCR can recognize and extract text within images, it does not provide meaningful descriptions of the visual elements. This is a critical limitation when attempting to perform QA on images or when seeking to provide image context for users. Moreover, text-to-text models alone would not be sufficient to perform the specific task of extracting image descriptions from PDF documents. A combination of text-to-text models and image-to-text models is essential for processing PDF documents with mixed text and visual content.\n",
    "\n",
    "### Pattern\n",
    "The pattern involves combining BLIP-2 and Claude in a multimodal QA system. BLIP-2 excels in image understanding, while Claude is a powerful text-based language model available via Bedrock API. The two models work synergistically to provide comprehensive answers to user queries, which can involve a combination of text and images. The proposed solution is to initiate with a PDF document, utilizing Blip-2 to process the images, generate image descriptions, and comprehend their visual content. Claude processes the textual content within the document. Users can then pose questions related to both the textual and visual aspects of the content. The system analyzes these queries, utilizes BLIP-2 for image description and Claude for text understanding, thereby facilitating comprehensive and intuitive question-answering. This approach enhances content extraction and QA on multimodal documents, making it valuable in fields like education, research, content curation, and more.\n",
    "\n",
    "\n",
    "### Challenges\n",
    "\n",
    "- Integrating BLIP-2 and Claude to handle both text and images in PDF documents.\n",
    "- Maintaining context between textual and visual elements to ensure meaningful answers in multimodal QA.\n",
    "\n",
    "### Proposal\n",
    "To the above challenges, this notebook proposes the following strategy\n",
    "\n",
    "![Diagram](./images/diagram.png)\n",
    "\n",
    "#### Deploy BLIP-2 to a SageMaker endpoint\n",
    "You can host an LLM on SageMaker using the Large Model Inference (LMI) container that is optimized for hosting large models using DJLServing. \n",
    "\n",
    "- Retrieve the Docker image of DJLServing\n",
    "- Package the inference script and configuration files as a model.tar.gz file\n",
    "- Upload the model.tar.gz file to Amazon S3 bucket\n",
    "- Create the model, the configuration for the endpoint, and the endpoint\n",
    "\n",
    "More details can be found in this blog: https://aws.amazon.com/it/blogs/machine-learning/build-a-generative-ai-based-content-moderation-solution-on-amazon-sagemaker-jumpstart/\n",
    "\n",
    "#### Load, split, and invoke the endpoint\n",
    "When the endpoint is deployed, you are ready to extract the text and image from a PDF, and invoke the endpoint to generate the image description and comprehend its visual content.\n",
    "\n",
    "- Load a pdf from local disk\n",
    "- Extract text and image\n",
    "- Invoke the BLIP-2 endpoint\n",
    "\n",
    "#### Invoke the Bedrock API for QA\n",
    "After extracting image description, you can invoke the Bedrock API with a predifined prompt template to produce organized QA responses based on user queries and the extracted content.\n",
    "\n",
    "- Create prompt templates\n",
    "- Invoke the Bedrock API \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403c458b-1c42-4a0d-98dc-750ff19c3740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker boto3 huggingface_hub --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ecae5-822a-487c-a58f-3a6f364eb508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import jinja2\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import json\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfc4d04-25f9-4c10-aadf-a76b57ba8cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55e63f5-ab27-4eb1-81ca-2e166483f1a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "s3_code_prefix = \"blip2\"  # folder within bucket where code artifact will go\n",
    "s3_model_prefix = \"model_blip2\"  # folder within bucket where code artifact will go\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "jinja_env = jinja2.Environment()\n",
    "\n",
    "# define a variable to contain the s3url of the location that has the model\n",
    "pretrained_model_location = f\"s3://{model_bucket}/{s3_model_prefix}/\"\n",
    "print(f\"Pretrained model will be uploaded to ---- > {pretrained_model_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7499bb-2944-4bdc-95a3-fd3ec3dee465",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare inference script and container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ef157d-64e9-4dff-9806-32f94062470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "try:\n",
    "\trole = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "\tiam = boto3.client('iam')\n",
    "\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'Salesforce/blip2-flan-t5-xxl',\n",
    "\t'HF_TASK':'image-to-text'\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\ttransformers_version='4.26.0',\n",
    "\tpytorch_version='1.13.1',\n",
    "\tpy_version='py39',\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1, # number of instances\n",
    "\tinstance_type='ml.m5.xlarge' # ec2 instance type\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ee875d-91d0-4ab8-901f-0a0bce95652b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\", region=sess.boto_session.region_name, version=\"0.22.1\"\n",
    ")\n",
    "inference_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53680769-4e06-4270-b376-eb3d5a71f3ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blip_model_version = \"blip2-flan-t5-xl\"\n",
    "model_names = {\n",
    "    \"caption_model_name\": blip_model_version, #@param [\"blip-base\", \"blip-large\", \"blip2-flan-t5-xl\"]\n",
    "}\n",
    "with open(\"blip2/model_name.json\",'w') as file:\n",
    "    json.dump(model_names, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1c3412-1df2-49d2-9cd0-31b2326389c4",
   "metadata": {},
   "source": [
    "In this notebook, we will provide two ways to load the model when deploying to an endpoint.\n",
    "- Directly load from Hugging Face \n",
    "- Store the model artifacts on S3 and load the model directly from S3\n",
    "\n",
    "The [Large Model Inference (LMI)](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-dlc.html) container uses [s5cmd](https://github.com/peak/s5cmd) to download data from S3 which significantly reduces the speed when loading model during deployment. Therefore, we recommend to load the model from S3 by following the below section to download the model from Hugging Face and upload the model on S3. \n",
    "\n",
    "If you choose to load the model directly from Hugging Face during model deployment, you can skip the below section and jump to the section to **prepare the model tarbal file and upload to S3**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7102d7-58e5-449e-8dc3-eb1bf76da79a",
   "metadata": {},
   "source": [
    "### [OPTIONAL] Download the model from Hugging Face and upload the model artifacts on Amazon S3\n",
    "If you intend to download your copy of the model and upload it to a s3 location in your AWS account, please follow the below steps, else you can skip to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaf38b0-5006-40e0-95be-8b1bbea48ce9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "CAPTION_MODELS = {\n",
    "    'blip-base': 'Salesforce/blip-image-captioning-base',   # 990MB\n",
    "    'blip-large': 'Salesforce/blip-image-captioning-large', # 1.9GB\n",
    "    'blip2-2.7b': 'Salesforce/blip2-opt-2.7b',              # 15.5GB\n",
    "    'blip2-flan-t5-xl': 'Salesforce/blip2-flan-t5-xl',      # 15.77GB\n",
    "}\n",
    "\n",
    "# - This will download the model into the current directory where ever the jupyter notebook is running\n",
    "local_model_path = Path(\"./blip2-model\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "model_name = CAPTION_MODELS[blip_model_version]\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.txt\", \"*.model\"]\n",
    "\n",
    "# - Leverage the snapshot library to donload the model since the model is stored in repository using LFS\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_model_path,\n",
    "    allow_patterns=allow_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2be006-576f-4799-a16c-f26a9da66b80",
   "metadata": {},
   "source": [
    "Please make sure the file is downloaded correctly by checking the files exist in the newly created folder `blip2-model/models--Salesforce--<model-name>/snapshots/...` before running the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec0aeb4-9e76-4b4b-ab69-f3aacecb32b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# upload the model artifacts to s3\n",
    "model_artifact = sess.upload_data(path=model_download_path, key_prefix=s3_model_prefix)\n",
    "print(f\"Model uploaded to --- > {model_artifact}\")\n",
    "print(f\"We will set option.s3url={model_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b8532e-ee56-4596-a55a-0666baac42a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf {local_model_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001f38dc-1d75-4167-a4f9-4518024c7bef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile blip2/serving.properties\n",
    "engine = Python\n",
    "option.tensor_parallel_degree = 1\n",
    "option.model_id = {{s3url}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eabd92-12d6-4f05-90fe-219c290e0b05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we plug in the appropriate model location into our `serving.properties` file based on the region in which this notebook is running\n",
    "template = jinja_env.from_string(Path(\"blip2/serving.properties\").open().read())\n",
    "Path(\"blip2/serving.properties\").open(\"w\").write(\n",
    "    template.render(s3url=pretrained_model_location)\n",
    ")\n",
    "!pygmentize blip2/serving.properties | cat -n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31c66dd-ee1d-4cc5-961b-0b9b063c4f22",
   "metadata": {},
   "source": [
    "## Prepare the model tarball file and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeae8bc2-f3c9-4174-8ebe-3230efaa72a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "tar czvf model.tar.gz blip2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec0e818-1a0a-4d5a-b86e-f06d19eaa0e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521fb573-b45c-4bbc-8f2e-0e46bff6f14a",
   "metadata": {},
   "source": [
    "## Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304c8205-c549-4f09-abab-7525047fab39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "model_name = name_from_base(blip_model_version)\n",
    "model = Model(\n",
    "    image_uri=inference_image_uri,\n",
    "    model_data=s3_code_artifact,\n",
    "    role=role,\n",
    "    name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fc1232-8d70-4ea1-a617-d43fb4eaab3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "endpoint_name = \"endpoint-\" + model_name\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b1b633-2839-42c4-a4e0-65568e8bd302",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af32882-3587-4832-9948-36d36d87b050",
   "metadata": {},
   "source": [
    "## Test Inference Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3987ea64-7038-4020-8144-8895fd072b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import base64\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "endpoint_name = model.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae73ace-0aea-4379-9083-471b168d62bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encode the image in base64\n",
    "def encode_image(img_file):\n",
    "    with open(img_file, \"rb\") as image_file:\n",
    "        img_str = base64.b64encode(image_file.read())\n",
    "        base64_string = img_str.decode(\"latin1\")\n",
    "    return base64_string\n",
    "\n",
    "# Run inference on the endpoint\n",
    "def run_inference(endpoint_name, inputs):\n",
    "    response = smr_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, Body=json.dumps(inputs)\n",
    "    )\n",
    "    return response[\"Body\"].read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79776b65-cb24-4b2b-af13-27ec74bbcd87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display a test image\n",
    "test_image = \"test_image.jpg\"\n",
    "raw_image = Image.open(test_image).convert('RGB')\n",
    "display(raw_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d5c005-5ca4-455f-b05b-8aec0aa3f8b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encode the test image and send it to the endpoint.\n",
    "base64_string = encode_image(test_image)\n",
    "inputs = {\"image\": base64_string}\n",
    "run_inference(endpoint_name, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67999f9-90bf-43f4-b910-6390e44ad06b",
   "metadata": {},
   "source": [
    "## Load pdf, split image and text, run inference on the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf7c00a-0b67-4168-bdf9-b6051ac5a925",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01dcbf2-7eb7-4ca1-b985-d5fafdd451f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ec4ab0-8f65-4252-b0d9-3df2383cd057",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function that extracts text and images from PDF\n",
    "def extract_text_and_images_from_pdf(filename):\n",
    "    try:\n",
    "        # Open the PDF file\n",
    "        pdf_document = fitz.open(filename)\n",
    "\n",
    "        # Initialize the text variable\n",
    "        text = ''\n",
    "\n",
    "        # Extract text\n",
    "        for page in pdf_document:\n",
    "            text += page.get_text()\n",
    "\n",
    "        # Print the extracted text\n",
    "        print(\"Extracted Text:\")\n",
    "        print(text)\n",
    "\n",
    "        # Initialize the image_filename variable\n",
    "        image_filename = ''\n",
    "\n",
    "        # Extract images\n",
    "        for page_number, page in enumerate(pdf_document, start=1):\n",
    "            images = page.get_images(full=True)\n",
    "            for index, image in enumerate(images):\n",
    "                xref = image[0]\n",
    "                base_image = pdf_document.extract_image(xref)\n",
    "                image_data = base_image[\"image\"]\n",
    "\n",
    "                # Assign a new value to image_filename within the loop\n",
    "                image_filename = f'{filename}.png'\n",
    "                with open(image_filename, \"wb\") as image_file:\n",
    "                    image_file.write(image_data)\n",
    "\n",
    "        # Print the image filename\n",
    "        print(\"Image Filename:\", image_filename)\n",
    "\n",
    "        # Close the PDF document\n",
    "        pdf_document.close()\n",
    "\n",
    "        return text, image_filename\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78157add-939a-4e5c-a55a-00e452110a70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get text and save the image on the local disk from a PDF\n",
    "filename='Tropical_Paradise.pdf'\n",
    "text, image_filename=extract_text_and_images_from_pdf(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2c6608-645a-4395-96ee-6245c7d36d1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Send the image to the endpoint\n",
    "base64_string = encode_image(image_filename)\n",
    "inputs = {\"image\": base64_string}\n",
    "image_description = run_inference(endpoint_name, inputs)\n",
    "image_description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d316a5-f5fc-42f7-bafc-1343852d1cc1",
   "metadata": {},
   "source": [
    "## Bedrock API for QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f0f43d-5cf6-4e93-a679-f3c1734ca94d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434ffc0f-62db-481f-bb35-d7be36470311",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "import botocore\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "\n",
    "bedrock_runtime = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6383261c-e7e0-47a0-8010-780c336d7c81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a prompt tenplate for summarization\n",
    "prompt_data = f\"\"\"\n",
    "Human: I am proving you an image description and a text. \n",
    "\n",
    "<image description>\n",
    "{image_description}\n",
    "<\\image description>\n",
    "\n",
    "<text>\n",
    "{text}\n",
    "<\\text>\n",
    "\n",
    "Please provide a summary of the image description and text.\n",
    "\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40394d76-9771-4a3e-97ab-701896605193",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Invoke the Bedrock API using the prompt template \n",
    "body = json.dumps({\"prompt\": prompt_data, \"max_tokens_to_sample\": 500})\n",
    "modelId = \"anthropic.claude-instant-v1\"  # change this to use a different version from the model provider\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "try:\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "    print(response_body.get(\"completion\"))\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    \n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180cbecb-8675-40c0-8fbb-7ad46d1db650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load a more complex pdf, split image and text, run inference on the image\n",
    "\n",
    "filename='WEF_Better_Climate_Better_Health_September_2023-23.pdf'\n",
    "text, image_filename=extract_text_and_images_from_pdf(filename)\n",
    "base64_string = encode_image(image_filename)\n",
    "inputs = {\"image\": base64_string}\n",
    "image_description = run_inference(endpoint_name, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6573218d-9cf8-4688-a190-50ff5e1f8c70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a prompt tenplate for QA\n",
    "prompt_data = f\"\"\"\n",
    "Human: I am proving you an image description and a text. \n",
    "\n",
    "<image description>\n",
    "{image_description}\n",
    "<\\image description>\n",
    "\n",
    "<text>\n",
    "{text}\n",
    "<\\text>\n",
    "\n",
    "Can you give me information about the climate governance of the country in the image?\n",
    "\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da5f5cd-77af-417c-a545-d610a435c7c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Invoke the Bedrock API for QA using the prompt template\n",
    "body = json.dumps({\"prompt\": prompt_data, \"max_tokens_to_sample\": 500})\n",
    "modelId = \"anthropic.claude-instant-v1\"  # change this to use a different version from the model provider\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "try:\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "    print(response_body.get(\"completion\"))\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    \n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c714b1b-00c6-48be-b0d0-2381bf5e6cf2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clean up\n",
    "Uncomment the below cell to delete the endpoint and model when you finish the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22616f1c-a919-4f77-af58-de3a0bbffe06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sm_client.delete_model(ModelName=model_name)\n",
    "# sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268285b1-866c-4191-a001-946b79ad5ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
